{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "\n",
    "# Pastikan stopwords NLTK telah diunduh\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inisialisasi Stopword dari Sastrawi dan NLTK\n",
    "factory = StopWordRemoverFactory()\n",
    "stop_words_sastrawi = set(factory.get_stop_words())\n",
    "stop_words_nltk = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Stopwords manual tambahan\n",
    "manual_stopwords = set([\n",
    "    \"rt\", \"shopee\", \"makinsayangshopee\", \"belanjadishopee\", \"shopeedihati\"\n",
    "])\n",
    "\n",
    "# Path file stopwords tambahan\n",
    "txt_stopword_path = r\"D:\\Tugas Akhir\\Final\\stopwords.txt\"\n",
    "txt_stopword = pd.read_csv(txt_stopword_path, names=[\"stopwords\"], header=None)\n",
    "txt_stopwords = set(txt_stopword[\"stopwords\"][0].split())\n",
    "\n",
    "# Gabungkan semua stopwords\n",
    "all_stopwords = stop_words_sastrawi.union(stop_words_nltk).union(manual_stopwords).union(txt_stopwords)\n",
    "\n",
    "# Fungsi untuk menghapus stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in all_stopwords]\n",
    "\n",
    "# Inisialisasi Stemmer\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "# Load Kamus Slang\n",
    "slang_dictionary_path = r\"D:\\Tugas Akhir\\Final\\Kamus Slang.csv\"\n",
    "slang_df = pd.read_csv(slang_dictionary_path, delimiter=';', encoding='latin1')\n",
    "slang_dict = dict(zip(slang_df['slang'], slang_df['normalisasi']))\n",
    "\n",
    "# Fungsi untuk normalisasi slang sebelum tokenisasi\n",
    "def normalize_slang(text):\n",
    "    words = text.split()\n",
    "    normalized_text = ' '.join([slang_dict.get(word, word) for word in words])\n",
    "    return normalized_text\n",
    "\n",
    "# Fungsi utama preprocessing\n",
    "def preprocess_text(text, scenario=1):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Case Folding\n",
    "    text = text.lower()\n",
    "\n",
    "    # Hapus URL\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Ganti username\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Ganti huruf berulang tiga kali atau lebih\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    # Hapus semua tanda baca\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Hapus karakter non-alfabet\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Hapus karakter tunggal\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # Normalisasi slang\n",
    "    text = normalize_slang(text)\n",
    "\n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Hapus stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "\n",
    "    # Gabungkan kembali token\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    # Stemming jika skenario = 2\n",
    "    if scenario == 2:\n",
    "        text = stemmer.stem(text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Load Dataset\n",
    "file_path = r\"D:\\Tugas Akhir\\Final\\tweetsfinal.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Validasi Kolom 'tweet'\n",
    "if 'tweet' not in df.columns:\n",
    "    raise ValueError(\"Kolom 'tweet' tidak ditemukan dalam dataset.\")\n",
    "\n",
    "# Ganti label\n",
    "df['label'] = df['label'].replace({\"Extraversion\": \"Extroversion\", \"Ekstraversion\": \"Extroversion\"})\n",
    "\n",
    "# Hapus retweet\n",
    "df = df[~df['tweet'].str.startswith('rt ', na=False)]\n",
    "\n",
    "# Batasi jumlah tweet\n",
    "df_limited = df.groupby(['username', 'label']).head(100).reset_index(drop=True)\n",
    "\n",
    "# Gabungkan tweet per pengguna\n",
    "df_grouped = df_limited.groupby(['username', 'label'])['tweet'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Terapkan Preprocessing untuk Setiap Skenario\n",
    "output_paths = [\n",
    "    r\"D:\\\\Tugas Akhir\\\\Final\\\\skenario_1_final.csv\",\n",
    "    r\"D:\\\\Tugas Akhir\\\\Final\\\\skenario_2_final.csv\",\n",
    "]\n",
    "\n",
    "for scenario in range(1, 3):\n",
    "    # Proses preprocessing\n",
    "    df_grouped['processed_text'] = df_grouped['tweet'].apply(lambda x: preprocess_text(x, scenario=scenario))\n",
    "\n",
    "    # Pilih kolom yang disimpan\n",
    "    df_subset = df_grouped[['username', 'label', 'processed_text']]\n",
    "\n",
    "    # Simpan hasil\n",
    "    df_subset.to_csv(output_paths[scenario - 1], index=False)\n",
    "    print(f\"Skenario {scenario} berhasil disimpan ke: {output_paths[scenario - 1]}\")\n",
    "\n",
    "print(\"\\nPreprocessing untuk semua skenario selesai.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
